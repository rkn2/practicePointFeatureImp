\documentclass[12pt]{article}
\usepackage{graphicx} % Required for inserting images
\usepackage{subcaption} % Required for subfigures
\usepackage{float} % Required for [H] figure placement
\usepackage{mathptmx} % Times-like font
\usepackage{setspace} % Required for setting line spacing
\usepackage[margin=1in]{geometry} % 1-inch margins all around
\usepackage{outlines}
\usepackage{comment}
\usepackage{hyperref} % For linking to notebooks
\usepackage{color,soul}
\usepackage[notes,backend=biber,endnotes]{biblatex-chicago}
\addbibresource{bib.bib}

\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=blue,
}

\title{Dimensionality Reduction for Heritage Preservation: Using Machine Learning to Prioritize Building Preservation Decisions}

\author{Rebecca Napolitano and Joe Kallas}
\date{December 2025}

\begin{document}

\maketitle

\noindent \textbf{Abstract:} Heritage practitioners now collect unprecedented volumes of data, such as environmental sensors, structural surveys, climate records, yet often struggle to translate this information into clear conservation priorities. 
This practice point demonstrates how machine learning techniques can identify the environmental and structural variables most strongly associated with deterioration, enabling practitioners to focus limited resources where they will have the greatest impact. 
We present factor analysis for exploratory pattern discovery and feature importance analysis for predictive modeling, with step-by-step interactive tutorials requiring no programming experience. 
These tools guide users from data upload through result interpretation, supporting evidence-based decision-making at scales ranging from individual components to entire districts.

\section{Introduction}

Picture this: you manage 200 historic buildings in a downtown district. 
You have climate data from 15 weather stations spanning two decades, structural surveys documenting crack patterns and material decay, soil moisture readings, temperature fluctuations, and documentation of every intervention.
Thousands of data points. Dozens of variables. One question: where do you start?

This scenario appears everywhere in preservation now. Digital documentation tools, environmental sensors, and GIS systems have become standard practice. 
We drown in data but starve for insights! 
The old approach of eyeballing a handful of variables and making gut-check comparisons breaks down when you juggle dozens of interrelated parameters.
Here is where dimensionality reduction techniques can help; these techniques take messy, complex datasets and distill them to what matters. 
Instead of examining each variable alone, they reveal underlying relationships and quantify which factors most influence outcomes: structural deterioration, material decay, climate vulnerability.

Data-driven analysis is not new to preservation; we have done quantitative work for decades \cite{mikesell1986, unesco2014}.
What has changed is scale and accessibility.
Modern machine learning algorithms process datasets impossible to handle twenty years ago \cite{amb2025}; cloud-based platforms like Google Colab \cite{googlecolab2025} eliminate the need for specialized software or expensive infrastructure; most important, we can now make these tools accessible to practitioners who have never written code \cite{neurond2024}.

This practice point introduces two complementary techniques: \textit{factor analysis} for exploratory pattern discovery and \textit{feature importance analysis} for predictive modeling. 
These serve distinct but related purposes. 
Factor analysis helps you understand the structure of your data, discovering, for example, that humidity, rainfall, and groundwater levels form a coherent ``moisture-related" pattern. Feature importance analysis addresses a different question: given a specific outcome (like condition rating), which variables best predict it?
% We use factor analysis rather than Principal Component Analysis (PCA) because preservation practitioners benefit from interpretable, rotated factors that align with physical deterioration mechanisms. While PCA maximizes variance explained, factor analysis with rotation (like Varimax) produces factors that more clearly correspond to real-world processes—moisture exposure, thermal cycling, structural aging. This interpretability matters when communicating findings to stakeholders and translating results into conservation strategies.
These techniques can be applied at any scale. 
At the component level, they identify which environmental conditions most affect specific materials or structural elements; at the building scale, they prioritize interventions based on factors threatening overall integrity; at the district or city scale, they allocate resources across multiple sites by identifying shared vulnerabilities and site-specific risks.

In this practice point, we start with project planning to help you determine whether these techniques suit your project and what data you need. 
Then we explain factor analysis and feature importance mechanics, with interactive notebooks guiding you through implementation. 
We bridge the gap between analytical outputs and actionable conservation plans in the interpretation section. 
We close by addressing common challenges and pointing you toward resources for continued learning.
Our goal is not to replace your preservation expertise with algorithms. 
Domain knowledge remains essential for framing questions, interpreting results, and translating findings into interventions. 
These computational tools help you extract more insight from data you already collect, so you can decide where to focus limited resources.

\section{Project Planning: Scoping Your Data-Driven Project}

Before you open a computational notebook or run algorithms, plan. Just as you would not start a structural stabilization project without assessing the building's condition and understanding failure mechanisms, do not jump into data-driven analysis without defining your objectives and evaluating your data.

\subsection{Defining Your Questions}

Dimensionality reduction techniques are powerful but not appropriate for every challenge. They excel when you face complexity: many variables, unclear relationships between factors, uncertainty about which parameters matter most \cite{wani2025, fodor2002}.
% Consider whether your project involves questions like those in Fig~\ref{fig:questions}. If you deal with only two or three variables with clear, well-understood relationships, traditional analytical methods will serve you better. 
% Dimensionality reduction becomes valuable when you juggle ten or more variables and the relationships are not obvious from domain knowledge alone.
% 
% \begin{comment}
% \begin{figure}[H]
%     \centering
%     \includegraphics[width=1\linewidth]{figures/questions.png}
%     \caption{Enter Caption}
%     \label{fig:questions}
% \end{figure}
% \end{comment}
% 
% \subsection{Data Requirements}
% 
% ``Garbage in, garbage out" applies especially to machine learning. The quality and characteristics of your input data determine the reliability of your results.
% % Dimensionality reduction works with diverse data types common in preservation practice (Fig~\ref{fig:datatypes}). 
% % The algorithms we discuss require numerical data. Categorical variables—``sandstone" versus ``limestone" or ``north-facing" versus ``south-facing"—must be converted to numerical representations through encoding. 
% % The tutorial notebooks demonstrate this.
% 
% \begin{comment}
% \begin{figure}
%     \centering
%     \includegraphics[width=1\linewidth]{figures/dataTypes.png}
%     \caption{Enter Caption}
%     \label{fig:datatypes}
% \end{figure}
% \end{comment}
Practical experience and statistical literature suggest minimum requirements.
For factor analysis, aim for at least 5-10 observations per variable (e.g., 100 observations for 10-20 variables) to achieve stable factor structures \cite{gracemartin2021}. 
For Random Forest models, 30 observations is the absolute minimum, but 100+ observations yield more reliable importance rankings \cite{wang2016}. With smaller samples, results become unstable and may not replicate \cite{calle2010}.

Beyond quantity, data quality matters enormously. 
Were all variables measured using the same protocols, instruments, and units? 
Inconsistent measurement methods introduce noise that obscures real patterns. 
Heritage data present specific challenges: temporal autocorrelation (measurements over time are not independent), spatial dependencies (buildings in the same district share exposures), and survivorship bias (severely deteriorated buildings may have been demolished, removing extreme cases from your dataset). 
These issues can lead to spurious correlations. 
Where possible, account for temporal structure by including time lags or using time-series methods, and for spatial structure by including location variables or using spatial statistics.
% \begin{figure}
%      \centering
%      \includegraphics[width=\linewidth]{minimums.png}
%      \caption{Enter Caption}
%      \label{fig:mins}
%  \end{figure}
Lastly, watch for outliers, extreme or erroneous data points that distort results. Examine them carefully before removing them; in heritage data, ``extremes" may represent significant events like storms or structural failures worth understanding rather than discarding.
Figure \ref{fig:condition_dist} shows the distribution of condition ratings in the example dataset for this practice point, which helps identify potential imbalances that could bias predictions.


\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\linewidth]{figures/condition_distribution.png}
    \caption{Distribution of building condition ratings in the dataset. Understanding the balance of your data (e.g., how many ``Good" vs. ``Poor" buildings) is essential before starting analysis. An imbalanced dataset can bias model predictions.}
    \label{fig:condition_dist}
\end{figure}


\subsection{Exploratory Data Visualization}
Before applying dimensionality reduction techniques, examine your data's structure through three complementary visualizations. The following exploratory visualizations (Figures \ref{fig:correlation}--\ref{fig:vif}) should be examined before proceeding to formal analysis, as they reveal data structure and potential issues requiring attention.

Start by examining correlation patterns (Figure \ref{fig:correlation}) to identify which variables move together; these visualizations are generated in the \texttt{04\_visualization.ipynb} notebook and the \texttt{01\_simple\_statistics.ipynb} notebook includes correlation analysis tools.
For a more detailed view of relationships, pairwise scatter plots (Figure \ref{fig:pairplot}) reveal non-linear patterns and outliers that simple correlation coefficients miss.
Before proceeding to analysis, check for multicollinearity using Variance Inflation Factor scores (Figure \ref{fig:vif}), which identify redundant variables that should be removed or combined; the \texttt{01\_simple\_statistics.ipynb} notebook provides VIF calculation and interpretation guidance. 


\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\linewidth]{figures/correlation_heatmap.png}
    \caption{Correlation matrix heatmap showing relationships between variables. Darker red indicates strong positive correlation, while darker blue indicates strong negative correlation. This high-level overview helps identify potential patterns, such as moisture variables correlating with condition.}
    \label{fig:correlation}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.85\linewidth]{figures/pairplot.png}
    \caption{Pairwise scatter plots showing relationships among key continuous variables. The diagonal shows distributions of individual variables, while off-diagonal plots reveal bivariate relationships. This visualization helps identify non-linear patterns and outliers that correlation coefficients alone might miss. Note that all variables have been standardized (scaled to mean = 0, standard deviation = 1) for comparability, which is why some variables show negative values.}
    \label{fig:pairplot}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\linewidth]{figures/vif_multicollinearity.png}
    \caption{Variance Inflation Factor (VIF) scores for predictor variables. VIF values above 5 or 10 indicate high multicollinearity, meaning variables are redundant (e.g., Average Temperature and Temperature Range). Identifying this early ensures statistical rigor.}
    \label{fig:vif}
\end{figure}

\section{The ``How-To": Data Featurization}

In this section we focus on  two complementary analytical techniques: \textit{factor analysis} and \textit{feature importance analysis}. 
While the mathematical details happen behind the scenes in the computational notebooks, understanding what these techniques do and when to use them is essential for interpreting results correctly and applying them appropriately to preservation decisions.
The workflow follows three stages. 
First, data preparation organizes your dataset, handles missing values, and standardizes variables for fair comparison. 
Second, analysis applies algorithms to identify patterns (factor analysis) or rank importance (feature importance). 
Third, interpretation translates numerical outputs into preservation insights (a process we detail in Section \ref{interpretation}). 
This section focuses on the analytical techniques themselves.

\subsection{Factor Analysis}

\subsubsection{What It Does}

Factor analysis explores patterns and relationships among parameters affecting structures, but note that these are \textit{correlational} patterns, not \textit{causal} mechanisms \cite{palomares2024, frost2023}. 
When you measure many variables such as temperature, humidity, wind exposure, material properties, structural dimensions, these variables are rarely independent.
Temperature and humidity vary together with seasonal patterns; wind exposure and salt deposition co-occur at coastal sites; soil moisture and groundwater levels fluctuate in coordination.

Factor analysis identifies clusters of variables that vary together, revealing hidden structures \cite{goretzko2021}. We identify and extract factors that \textit{correlate with} structural behavior. 
These factors represent overarching patterns that account for observed correlations among parameters. 
Understanding \textit{why} these correlations exist requires domain expertise; the statistical method only reveals \textit{that} they exist.

Think of factors as categories or groups of conditions that collectively \textit{correlate with} structural behavior. These conditions are not considered alone but as part of broader, interrelated themes. Instead of separately tracking ten moisture-related variables (perhaps rainfall, humidity, condensation, groundwater level, soil saturation) factor analysis might reveal that these cluster into two meaningful factors: ``atmospheric moisture" and ``ground moisture." 
This consolidation simplifies subsequent analysis while preserving essential information.
Each factor corresponds to a specific set of related conditions; parameters within the same factor share common characteristics in how they \textit{co-vary with} structural impacts. 
The technique assigns a \textit{loading} to each variable for each factor, a weight indicating how strongly that variable contributes to that factor. 
High loadings (typically above 0.6-0.7) indicate strong relationships; low loadings suggest the variable is not part of that factor's pattern.

Factor analysis reduces dimensionality by condensing numerous parameters into fewer meaningful factors. 
If you begin with 30 variables, factor analysis might identify 5-8 underlying factors that capture 70-80\% of the variation. 
This simplification focuses understanding on critical conditions \textit{associated with} structural changes. 
Rather than monitoring and responding to 30 different variables, you focus conservation efforts on the handful of factors that show the strongest associations with deterioration patterns.
The results are interpretable factors that represent underlying structural behavior drivers. 
These factors are derived mathematically, but \textit{you} as the preservation professional give them meaning based on domain expertise. 
The algorithm identifies which variables cluster together; you name the factors. 
A factor with high loadings on temperature range, freeze-thaw cycles, and thermal expansion might be named ``thermal stress;" a factor loading heavily on wind speed, rainfall intensity, and exposed surface area might be termed ``weather exposure."

\subsubsection{When to Use Factor Analysis}

Factor analysis is particularly valuable in several preservation scenarios. 
First, it can be useful when you have collected many variables but are uncertain which ones are most relevant or how they relate to each other. 
Secondly,  it can be useful when ongoing monitoring collects numerous parameters but you need a smaller, more communicable set of metrics for practical decision-making. 
It is additionally useful when identifying deterioration mechanisms in the case where multiple environmental or structural stresses may be acting on a building, but you want to understand which combinations of stresses are most significant. 
Lastly, it can be useful when studying multiple buildings or sites and wanting to determine whether they experience similar or different patterns of stress. 
Factor analysis is less appropriate when you have few variables (fewer than 8-10), when your variables are largely independent, or when you already have strong theoretical reasons to know which variables matter most. 
In that last case, feature importance analysis (discussed next) may suit you better.

\subsubsection{The Process}

Factor analysis proceeds through several steps \cite{williams2010}. 
First, variables are standardized to ensure equal influence regardless of units. 
Second, the algorithm computes correlations between all variable pairs. 
Third, it extracts factors that explain correlation patterns. The first factor explains the most variance, the second explains the next-most, and so forth. 
Fourth, diagnostic tools determine how many factors to retain. The scree plot (Figure \ref{fig:scree}) shows where additional factors stop adding meaningful information.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\linewidth]{figures/scree_plot.png}
    \caption{Scree plot used to determine the number of factors to retain. The ``elbow" in the curve (where the line flattens out) suggests the optimal number of factors that capture the most meaningful variation in the data.}
    \label{fig:scree}
\end{figure}
Fifth, you interpret and name the factors based on which variables load strongly (typically 0.6 or higher). 
If solar radiation, temperature, and thermal expansion load on one factor, label it ``Thermal Stress" (Figure \ref{fig:loadings}). 
Each observation receives a factor score showing how strongly it exhibits each factor.
\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\linewidth]{figures/factor_loadings.png}
    \caption{Factor loadings heatmap. This visualization is key to interpretation, showing which variables cluster together. For example, Factor 1 might show high loadings for moisture-related variables, while Factor 2 groups thermal variables.}
    \label{fig:loadings}
\end{figure}
Once factors are identified and named, you can visualize how individual buildings score on each factor.
Figure \ref{fig:factor_scores} shows buildings plotted by their scores on the first two factors, color-coded by district.
Clustering patterns reveal whether certain districts share similar deterioration profiles, while scattered points indicate diverse conditions within districts.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.75\linewidth]{figures/factor_scores_scatter.png}
    \caption{Factor scores scatter plot showing how buildings distribute across the first two factors, color-coded by district. Clustering patterns reveal whether certain districts share similar deterioration profiles, while scattered points indicate diverse conditions within districts.}
    \label{fig:factor_scores}
\end{figure} 


\subsubsection{Interactive Tutorial: Factor Analysis}

The companion notebook (\texttt{02\_factor\_analysis.ipynb}) guides you through the complete workflow: data upload, statistical suitability testing, scree plot visualization, automatic factor number suggestion, Varimax rotation, and interpretation of factor loadings. 
The notebook uses a synthetic dataset of 200 heritage buildings with 9 environmental and structural variables, includes troubleshooting guidance, and saves results as CSV files for further analysis. 


\subsection{Feature Importance}

\subsubsection{What It Does}

While factor analysis identifies groups of related variables, feature importance analysis ranks individual variables by their predictive power for specific outcomes \cite{saarela2021} (deterioration severity, maintenance costs, intervention timing). The technique trains machine learning models (in this case Random Forest \cite{breiman2001} or Gradient Boosting \cite{friedman2001}) to learn relationships between input variables and target outcomes. 
These ensemble methods construct multiple decision trees and aggregate results to determine which parameters matter most \cite{zhou2021}.
Unlike simpler statistical approaches, these models automatically capture variable interactions \cite{basu2018}. 
High humidity alone may not predict deterioration, but combined with temperature fluctuations it might strongly predict decay. 
After training, the models measure how much predictive power would be lost if each variable were removed. 
Variables whose removal causes large performance drops are deemed more important. Importance scores range from 0 to 1, with higher values indicating greater predictive power.

The ranking generated by these machine learning models guides the prioritization of conditions and factors within each category. 
Parameters identified as highly significant could be given priority attention in subsequent preservation and intervention strategies \cite{KALLAS2024, Kallas2025a, KALLAS2025b}. 
If the analysis reveals that groundwater level is the strongest predictor of foundation deterioration, monitoring and managing groundwater can become the top priority.

Feature importance analysis is an iterative process, where the models are trained and evaluated using historical data and domain expertise. 
The process uses part of your data to build the model (the training set) and reserves part to test how well predictions work on new data (the test set). 
This ensures that the rankings are refined and reliable for decision-making rather than simply describing patterns specific to your particular dataset.
Feature importance contributes to dimensionality reduction not by grouping variables together (as factor analysis does), but by allowing you to filter out the least influential ones. 
This simplifies both interpretation and data collection since once you know which features drive outcomes, you can prioritize those in future monitoring campaigns or predictive models for this specific case of analysis.

\subsubsection{When to Use Feature Importance}

Feature importance analysis is particularly valuable in several scenarios. 
First, use it when you have a clear outcome to predict. 
The technique requires that you have both input variables (environmental conditions, material properties, structural characteristics) and an outcome variable you want to predict or explain (deterioration rating, intervention urgency, structural performance). 
If you only have input variables without outcomes, factor analysis is more appropriate.
Second, this technique shines when prioritization is the goal. 
When you need to rank variables to guide resource allocation, monitoring priorities, or intervention strategies, questions like ``Which three factors should we monitor most closely?" and ``Where should we spend our limited maintenance budget?" are ones that feature importance can help answer.
Third, use this technique when complex relationships are suspected. 
If multiple variables interact in nonlinear ways, traditional regression assumes linear relationships and requires you to specify interactions. 
Random Forests and Gradient Boosting automatically detect nonlinear patterns and interactions.
Lastly, this method matters when predictive accuracy counts. 
If you want not just to understand relationships but also to predict future outcomes (``Given current conditions, which buildings will require intervention in the next five years?").

Feature importance is less appropriate when you lack outcome data, when you have very few observations (fewer than 30), or when relationships are simple enough that traditional statistical methods suffice \cite{gulati, ewald2024}. 
It's also less useful when variables are highly collinear, when several variables measure nearly the same thing, because the model may distribute their importance unevenly.
When variables are highly correlated (multicollinear), feature importance rankings become unstable. 
You can check for multicollinearity by calculating Variance Inflation Factor (VIF) for each variable. Values above 10 indicate severe collinearity \cite{pennstate2025}. 
In such cases, consider removing redundant variables (e.g., keep annual rainfall but remove monthly rainfall averages), using factor scores from factor analysis as inputs, or accepting that importance rankings may vary but focusing only on the top 3-5 features that consistently rank high across different model runs.
The notebooks implement both Random Forests and Gradient Boosting, which work similarly but have different strengths explored in the notebook. 
We recommend running both models and comparing their feature importance rankings. 
If both models identify the same top variables as most important, you can be confident in those findings. 
If the models disagree substantially, that suggests either data quality issues or complex relationships that merit closer investigation.

\subsubsection{The Process}

While the provided tutorial notebook largely automates the mechanics of feature importance analysis, interpretation of the results requires a clear understanding of the underlying logic. 
The process begins with the definition of a target variable which must be cast in a numerical or ordinal format; this can be a measurable outcome such as damage severity, deterioration rate, or intervention cost. 
Simultaneously, feature variables representing environmental factors, material characteristics, or geometric properties are selected and standardized to ensure consistent numeric formatting.
A distinction in heritage data analysis is the strategy for splitting data into training and testing sets. 
Unlike standard random splits, which can introduce data leakage, heritage datasets often necessitate temporal or spatial partitioning. 
This may involve training on historical periods (e.g., 2010--2017) to test on subsequent years (2018--2020), or training on specific geographic districts to test generalization to others. 
Once partitioned, ensemble algorithms like Random Forest and Gradient Boosting are employed. 
These methods construct multiple decision trees to capture the complex, non-linear interactions inherent in the built environment.
Validation is then performed to assess prediction accuracy against domain-specific benchmarks. 
In the context of heritage datasets, an $R^2$ value between $0.4$ and $0.6$ typically indicates useful relationships, whereas a value above $0.7$ is excellent but uncommon.
Figure \ref{fig:regression} shows actual versus predicted values for a multivariate model, while Figure \ref{fig:simple_regression} demonstrates the limited explanatory power of single-variable approaches.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\linewidth]{figures/multiple_regression.png}
    \caption{Actual vs. Predicted values from a regression model. Points clustering closely around the red dashed line indicate accurate predictions. Outliers far from the line represent buildings where the model's prediction differed significantly from reality.}
    \label{fig:regression}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.65\linewidth]{figures/simple_regression.png}
    \caption{Simple linear regression of Condition Rating vs. Crack Width. This baseline model using a single predictor demonstrates why multivariate approaches are necessary. The scattered points show that crack width alone explains only part of the variation in building condition.}
    \label{fig:simple_regression}
\end{figure} 
Similarly, a Mean Absolute Error (MAE) of $0.5$ rating points on a standard $1$--$5$ scale is considered useful for prioritization, even if it lacks the precision required for individual diagnosis. 
To ensure the stability of these rankings, $5$-fold or $10$-fold cross-validation is recommended \cite{bhagat2025}.
Finally, feature importance is computed by measuring the performance loss occurred when a specific variable is removed (permutation importance \cite{saarela2021}). 
These results, ideally visualized as bar charts (Figure \ref{fig:importance}), should be interpreted in conjunction with preservation expertise.
To validate that importance rankings reflect true predictive power rather than model artifacts, permutation importance (Figure \ref{fig:permutation}) measures performance loss when each variable is shuffled.
Finally, comparing multiple model types (Figure \ref{fig:model_comp}) justifies algorithm selection and reveals whether simpler approaches might suffice.
\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\linewidth]{figures/feature_importance.png}
    \caption{Feature importance ranking. This chart directly answers ``What drives deterioration?" by ranking variables based on their predictive power. In this example, variables like Soil Moisture and Freeze-Thaw Cycles might appear as top drivers.}
    \label{fig:importance}
\end{figure}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\linewidth]{figures/permutation_importance.png}
    \caption{Permutation importance scores showing the decrease in model performance when each variable is randomly shuffled. This validation technique confirms that the ranked variables truly contribute to predictions rather than being artifacts of the model's internal structure.}
    \label{fig:permutation}
\end{figure}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\linewidth]{figures/model_comparison.png}
    \caption{Comparison of model performance ($R^2$ scores). This justifies the choice of algorithm; for instance, Random Forest often outperforms simple Linear Regression on complex heritage datasets by capturing non-linear relationships.}
    \label{fig:model_comp}
\end{figure}
Furthermore, utilizing the uncertainty estimates provided by algorithms like Random Forests allows for reporting predictions with necessary confidence intervals (e.g., ``$2.5 \pm 0.4$''), resulting in a more transparent analytical output.

\subsubsection{When Interpreting Results}

Never treat computational outputs as infallible; always validate results through site inspections and material testing. 
If model outputs conflict with field observations, re-examine your data. Feature importance provides rankings of influence, not proof of causation. 
High importance indicates correlation, not necessarily direct causation.
Combine computational results with field expertise; view rankings as guides for investigation, not definitive conclusions. High-importance variables often cluster together, indicating broader mechanisms like moisture-related stress. 
The value lies in prioritization: deciding where to focus monitoring or intervention.

\subsubsection{Interactive Tutorial: Feature Importance}

The companion notebook (\texttt{03\_feature\_importance.ipynb}) walks through model training, validation, and interpretation: train-test splitting with temporal/spatial considerations, hyperparameter explanations, dual model training (Random Forest and Gradient Boosting), performance evaluation, and feature importance visualization. The notebook includes model comparison tables, troubleshooting guidance for common issues, and saves all results as CSV files.

\subsubsection{Combining Factor Analysis and Feature Importance}

Factor analysis and feature importance are complementary. 
Factor analysis reveals hidden structures like groups of related parameters representing broader processes like ``moisture exposure" or ``thermal stress." 
Feature importance quantifies how strongly each variable predicts specific outcomes. 
Used together, they can transform complex datasets into actionable insights.
In terms of protocol, you can begin with factor analysis to identify underlying factors and inform hypotheses about relevant physical processes. 
Then you can apply feature importance to rank specific variables driving observed outcomes. 
When variables grouped by factor analysis also rank highly in feature importance, confidence in interpretation can increase whereas discrepancies can prompt further investigation.

\section{Implementation: Interpreting and Using the Results} \label{interpretation}

Running algorithms represents only the initial phase; the more significant challenge involves translating quantitative outputs into decisive action.
While computational models excel at highlighting patterns, their findings require validation through professional judgment and comparison with field observations. 
To operationalize these results, practitioners can rank sites by risk using factor scores or predicted outcomes, establishing specific thresholds that trigger intervention. 
However, these data-driven priorities must be confirmed via site visits, as models may overlook recent repairs or specific local conditions.
Furthermore, final decisions must weigh statistical risk against practical constraints like cost, technical feasibility, and cultural significance. 
Computational analysis should be viewed as a tool that supports, rather than supplants, core preservation principles.


\subsection{Decision Framework: Reconciling Models with Field Observations}

When translating model outputs into intervention priorities, visualizations help identify patterns that inform decision-making.
Figure \ref{fig:material_boxplot} shows how condition ratings vary by material type, revealing material-specific vulnerabilities.
Figure \ref{fig:spatial} maps predicted risk across building locations; geographic clustering of high-risk buildings suggests shared environmental exposures that warrant district-level rather than building-by-building interventions.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\linewidth]{figures/boxplot_condition_by_material.png}
    \caption{Distribution of condition ratings by material type. Box plots reveal median values (center line), quartiles (box edges), and outliers (points). Such visualizations help practitioners identify material-specific vulnerabilities and prioritize interventions accordingly.}
    \label{fig:material_boxplot}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.75\linewidth]{figures/spatial_map.png}
    \caption{Spatial distribution of predicted condition ratings across building locations. Geographic clustering of high-risk buildings suggests shared environmental exposures (e.g., groundwater, pollution) that warrant district-level interventions rather than building-by-building approaches.}
    \label{fig:spatial}
\end{figure}

When model outputs conflict with field observations or domain knowledge, work through the following steps. 
First, quantify the disagreement. 
How large is the discrepancy? 
If a model ranks a building as low-risk but you observe active deterioration, document specifically what the model missed (recent water infiltration, localized spalling, etc.).
Second, check data quality. 
Were the variables measured correctly for that building? 
Common issues include sensors placed in unrepresentative locations, measurements taken before recent damage occurred, data entry errors, or missing variables that would explain the discrepancy.
Third, consider temporal mismatches. 
Models trained on historical data may not capture recent changes. 
A building may have been in good condition when measured but deteriorated since. 
Conversely, recent repairs may have improved condition beyond what historical patterns predict.
Fourth, evaluate model limitations. 
Is the disagreement within the model's expected error range? 
Fifth, weight evidence by strength. 
As a rough guide, assign approximate weights: direct field observation by a qualified professional (40\%), model prediction with R² $>$ 0.7 and large sample (30\%), historical deterioration records (20\%), and model prediction with R² $<$ 0.5 or small sample (10\%). 
These are guidelines, not rules; adjust based on data quality and expertise available.
Finally, make a decision and document your reasoning. 
If field evidence outweighs model output, trust the field evidence but investigate why the model failed since this reveals data gaps or model limitations. 
If model output is strong and field evidence is ambiguous, use the model to prioritize further investigation; always document your reasoning for future reference and accountability.

\section{Challenges and Tips}

Applying dimensionality reduction and feature importance analyses in real preservation projects requires care and judgment. 
Heritage data are rarely ideal since they're often sparse, heterogeneous, and collected under varying conditions over long time spans. 
The goal is not to produce perfect models, but to extract useful patterns that support informed decision-making. 
Recognizing the limitations of both data and algorithms helps ensure that computational analysis enhances, rather than distorts, professional expertise.

Heritage datasets are often incomplete, with missing values scattered throughout. 
Imputation methods can help, but you should interpret results cautiously when more than 10-15\% of your data required imputation. 
Complex models may fit noise rather than signal, a problem known as overfitting. 
The best defense is validation: always test your model on data it has never seen. 
If performance drops sharply on test data compared to training data, your model has likely memorized idiosyncrasies rather than learned generalizable patterns.

A highly accurate model has little value if you cannot explain its reasoning to practitioners or stakeholders. 
This matters especially in preservation, where decisions must often be justified to review boards, funding agencies, or the public. 
Favor transparent algorithms over black-box models when possible. 
Computational results should complement (not replace!) condition surveys and material analyses. 
The most robust decisions combine algorithmic insights with field observations and expert judgment.

Document your data sources, methods, and parameter settings so others can reproduce and verify your results.
Transparency builds trust in data-driven preservation decisions and protects you if results are later questioned. 
What matters at the component level (moisture content in a specific wall) may differ from what matters at the district scale (groundwater rise affecting multiple buildings). 
Always interpret findings within the appropriate spatial and temporal context.

Models require periodic updating as new data accumulate. 
A reasonable schedule is annual retraining if you collect continuous monitoring data, or retraining when new data exceed 20-30\% of your original training set. Monitor prediction errors over time. 
If MAE increases by 50\% or more, your model may have become outdated due to changing conditions, a phenomenon called concept drift. 
For datasets spanning decades, recent 10-15 year rolling windows often perform better than cumulative data because they prioritize current conditions over historical patterns that may no longer apply.

% \subsection{When NOT to Use These Tools}}
% redundant 

% These methods are not appropriate for every situation. Recognize when simpler approaches or different methods serve you better.
% With fewer than 50 observations, or with more than 30\% missing data, results become unreliable. Simple correlation analysis or expert judgment may be more appropriate in these cases. 
% If your data have serious quality issues like inconsistent measurement protocols, unknown sensor calibration, mixed units then you must fix the data collection process before attempting sophisticated analysis. 
% No algorithm can compensate for fundamentally flawed data.

% If you have only 2-3 well-understood variables with clear relationships to outcomes, traditional regression or simple comparison methods will be faster, more transparent, and easier to explain to stakeholders. 
% Machine learning adds complexity without benefit in such straightforward cases. 
% Similarly, these models cannot reliably predict outside the range of conditions in your training data. 
% If your historical data cover moderate climate conditions (10-25°C), the model will not help predict deterioration under extreme heat (35°C+) or unprecedented freeze events. 
% For climate change scenarios or novel stressors, physics-based models or expert judgment are more appropriate.

% If you need to understand \textit{why} deterioration occurs or test specific hypotheses about mechanisms, these pattern-matching tools are insufficient. 
% They identify correlations, not causes. Material science experiments, controlled interventions, or causal inference methods (like propensity score matching or instrumental variables) are required for causal questions. 
% Do not use these tools alone to justify demolition, major structural interventions, or other irreversible actions. 
% They are hypothesis-generating tools, not decision-making oracles. 
% Always combine computational analysis with site inspections, material testing, structural engineering assessment, and peer review.

% If algorithmic recommendations could expose you to professional liability, proceed with extreme caution. 
% Document all assumptions, validate results extensively, and ensure human expert oversight. 
% In legal or regulatory contexts, simpler and more transparent methods may be preferable. 
% Finally, if you lack time to properly validate results, expertise to interpret outputs carefully, or resources to collect additional data when models reveal gaps, simpler methods may serve you better. 
% A well-executed simple analysis beats a poorly executed sophisticated one.

\section{Interactive Tutorials}

Five Google Colab notebooks provide hands-on training for practitioners with no programming experience. All code and data are available in the project repository: \url{https://github.com/rkn2/practicePointFeatureImp}. Start with Notebook 00 and work through sequentially.

\textbf{Data Preparation} (\href{https://colab.research.google.com/github/rkn2/practicePointFeatureImp/blob/main/00_data_preparation.ipynb}{\texttt{00\_data\_preparation.ipynb}}): File upload, data dictionary, handling missing values, one-hot encoding, standardization; includes optional multicollinearity check (VIF); outputs \texttt{processed\_data.csv}. Start here to clean and prepare your data.

\textbf{Simple Statistical Methods} (\href{https://colab.research.google.com/github/rkn2/practicePointFeatureImp/blob/main/01_simple_statistics.ipynb}{\texttt{01\_simple\_statistics.ipynb}}): Correlation analysis, simple and multiple regression, group comparisons, VIF calculation with solutions for multicollinearity. Use this when you have fewer than 50 observations, 2--5 variables, or need transparent methods for stakeholder communication.

\textbf{Factor Analysis} (\href{https://colab.research.google.com/github/rkn2/practicePointFeatureImp/blob/main/02_factor_analysis.ipynb}{\texttt{02\_factor\_analysis.ipynb}}): Bartlett's and KMO tests, scree plot, Varimax rotation, factor loadings; includes discussion of Factor Analysis vs. PCA and rotation choice guidance (Varimax vs. Promax); outputs \texttt{factor\_scores.csv} and \texttt{factor\_loadings.csv}.

\textbf{Feature Importance} (\href{https://colab.research.google.com/github/rkn2/practicePointFeatureImp/blob/main/03_feature_importance.ipynb}{\texttt{03\_feature\_importance.ipynb}}): Train-test split with temporal/spatial considerations, Random Forest and Gradient Boosting training, performance evaluation, model comparison; includes guidance on alternative models (Ridge, Lasso, Elastic Net, SVM, Neural Networks, KNN) with decision guide and learning resources; outputs \texttt{feature\_importance.csv} and \texttt{model\_comparison.csv}.

\textbf{Visualization} (\href{https://colab.research.google.com/github/rkn2/practicePointFeatureImp/blob/main/04_visualization.ipynb}{\texttt{04\_visualization.ipynb}}): Correlation heatmaps, distribution plots, boxplots, pairplots, summary statistics; saves all plots as 300 DPI PNG files for publication.

All notebooks include beginner-friendly explanations, error handling, troubleshooting sections, and ``Next Steps" guidance. 
The synthetic dataset includes 200 heritage buildings with 15 variables (construction year, district, material type, foundation type, temperature, rainfall, humidity, freeze-thaw cycles, soil moisture, crack width, salt deposition, condition rating, intervention urgency).




\end{document}